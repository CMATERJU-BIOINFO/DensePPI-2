{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ppin-1/miniconda3/envs/pytorch_gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "__file__ = os.getcwd() + \"/\"\n",
    "from os.path import dirname, join, abspath\n",
    "sys.path.insert(0, abspath(join(dirname(__file__), '..')))\n",
    "project_base_dir = abspath(join(dirname(__file__), '..'))\n",
    "\n",
    "from Utils.image_utils import *\n",
    "from Utils.path_utils import *\n",
    "from Utils.data_read_utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice for this session :\n",
      "1 channel image saved using plt.imsave()...\n",
      "Look Up Table Shape :  (20, 20)\n"
     ]
    }
   ],
   "source": [
    "model_saved_loc = \"saved_models/complete_epoch_010_metric_0.91918.pth.tar\"\n",
    "outputSaveBasePath = \"currTestSCnegP16pos/\"\n",
    "test_img_parent_loc = outputSaveBasePath + \"testImagesOrig/\"\n",
    "test_img_sub_loc = outputSaveBasePath + \"testImagesSub/test_sub/\"\n",
    "test_sub_img_dict = outputSaveBasePath + \"test_image_dict.json\"\n",
    "test_sub_img_prefix = \"test\"\n",
    "\n",
    "createLocationIfNotExists(outputSaveBasePath)\n",
    "createLocationIfNotExists(test_img_parent_loc)\n",
    "createLocationIfNotExists(test_img_sub_loc)\n",
    "substituton_matrix_init(outputSaveBasePath, 'pam', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAmat_loc = \"{}/input/sCerevisiaeData/P_protein_A.mat\".format(project_base_dir)\n",
    "NBmat_loc = \"{}/input/sCerevisiaeData/P_protein_B.mat\".format(project_base_dir)\n",
    "\n",
    "P_prefix = \"SCposP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAmat = scipy.io.loadmat(NAmat_loc)\n",
    "PBmat = scipy.io.loadmat(NBmat_loc)\n",
    "\n",
    "P1 = [PAmat['P_protein_A'][i, 0][0] for i in range(len(PAmat['P_protein_A']))]\n",
    "P2 = [PBmat['P_protein_B'][i, 0][0] for i in range(len(PBmat['P_protein_B']))]\n",
    "\n",
    "df = pd.DataFrame(columns=['Index', 'Protein_1', 'Protein_2', \n",
    "            'Protein_seq1', 'Protein_seq2'])\n",
    "total_P = copy.deepcopy(P1)\n",
    "total_P.extend(copy.deepcopy(P2))\n",
    "P_unique = np.unique(np.array(total_P))\n",
    "P_name_list = [P_prefix + str(i + 1) for i in range(P_unique.shape[0])]\n",
    "P_dict = {key : val for (key, val) in zip(P_unique, P_name_list)}\n",
    "count = 1\n",
    "for i in range(len(P1)):\n",
    "    li = list()\n",
    "    li.append(count)\n",
    "    li.append(P_dict[P1[i]])\n",
    "    li.append(P_dict[P2[i]])\n",
    "    li.append(P1[i])\n",
    "    li.append(P2[i]) \n",
    "    df = df._append(pd.Series(li, index=df.columns), ignore_index=True)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_out_loc = outputSaveBasePath + \"sc_pos_seq_dict.json\"\n",
    "\n",
    "with open(dict_out_loc,\"w\") as outfile:\n",
    "    json.dump(P_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================CONSTRAINTS ON DATA======================\n",
      "Maximum allowed sequence length :  inf\n",
      "Minimum allowed sequence length :  128\n",
      "===============================================================\n",
      "Before cleaning .....\n",
      "Total Interactions :  500\n",
      "After cleaning .....\n",
      "Total Interactions :  446\n"
     ]
    }
   ],
   "source": [
    "maxAllowedSequenceLength = np.inf\n",
    "print(\"======================CONSTRAINTS ON DATA======================\")\n",
    "print(\"Maximum allowed sequence length : \", maxAllowedSequenceLength)\n",
    "print(\"Minimum allowed sequence length : \", SIZE)\n",
    "print(\"===============================================================\")\n",
    "print(\"Before cleaning .....\")\n",
    "print(\"Total Interactions : \", len(df))\n",
    "\n",
    "mask = lambda x : checkChars(x['Protein_seq1'], x['Protein_seq2'])\n",
    "df = df.loc[df.apply(mask, axis = 1)]\n",
    "\n",
    "mask = (df['Protein_seq1'].str.len() >= SIZE) & (df['Protein_seq2'].str.len() >= SIZE) & (df['Protein_seq1'].str.len() < maxAllowedSequenceLength) & (df['Protein_seq2'].str.len() < maxAllowedSequenceLength)\n",
    "df = df.loc[mask]\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(\"After cleaning .....\")\n",
    "print(\"Total Interactions : \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(len(df)):\n",
    "    save_one_image_1C(it, df.iloc[it, 1], df.iloc[it, 2], df.iloc[it, 3], df.iloc[it, 4], test_img_parent_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_img_list = os.listdir(test_img_parent_loc)\n",
    "args = []\n",
    "for img in pos_test_img_list:\n",
    "    args.append(test_img_parent_loc + img)\n",
    "    \n",
    "imgCounter = SubImageCounter()\n",
    "\n",
    "num_workers = multiprocessing.cpu_count()  \n",
    "\n",
    "with multiprocessing.Pool(processes = num_workers) as pool:\n",
    "    for images in pool.map(handle_one_image, args):\t\t\t\t#Synchronus but in parallel\n",
    "        image_list = []\n",
    "        for image in images:\t\t\n",
    "            image_list.append(imgCounter.subimage_counter)\n",
    "            file_name = str(test_img_sub_loc + test_sub_img_prefix + \"_sub_\" + str(imgCounter.subimage_counter) + \".png\")\n",
    "            plt.imsave(file_name, image)\n",
    "            imgCounter.subimage_counter += 1\n",
    "        imgCounter.image_dict[imgCounter.image_counter] = image_list\n",
    "        imgCounter.image_counter += 1\n",
    "\n",
    "with open(test_sub_img_dict,\"w\") as outfile:\n",
    "    json.dump(imgCounter.image_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cuda:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sub_images_parent_loc = os.path.join(os.path.abspath(test_img_sub_loc), '..')\n",
    "test_data = datasets.ImageFolder(test_sub_images_parent_loc, transform=transformations)\n",
    "if (len(test_data.classes) == 1): #considering only negative\n",
    "    test_data.samples = [(d, 0) for d, _ in test_data.samples]\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=bs)\n",
    "model = PPIModel()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(\"device : \", device)\n",
    "model = model.to(device=device)\n",
    "checkpoint = torch.load(model_saved_loc, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting : 100%|██████████| 13832/13832 [21:04<00:00, 10.94batch/s, tst_acc=0.227, tst_loss=2.19] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction loss : 3.529438820343837 | Prediction Accuracy : 0.1088672840322474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "gr_tr = []\n",
    "pred = []\n",
    "raw_pred = []\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "\n",
    "with torch.no_grad(), tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "    tepoch.set_description(f\"Predicting \")\n",
    "    for input, labels in tepoch:\n",
    "        input, labels = input.to(device), labels.to(device)\n",
    "        logits = model.forward(input)\n",
    "        loss = criterion(logits, labels)\n",
    "        test_loss += loss.item()\n",
    "        ps = torch.exp(logits)\n",
    "        _, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        acc = torch.mean(equals.type(torch.FloatTensor)).item()  \n",
    "        gr_tr.extend(labels.tolist())\n",
    "        raw_pred.extend(logits.tolist())\n",
    "        pred.extend(top_class.squeeze().tolist())\n",
    "        test_acc += acc\n",
    "        tepoch.set_postfix(tst_loss = loss.item(), tst_acc = float(acc))\n",
    "\n",
    "print(\n",
    "    f\"Prediction loss : {test_loss/len(test_dataloader)} | \"\n",
    "    f\"Prediction Accuracy : {test_acc/len(test_dataloader)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_labels, predictions, raw_preds = np.array(gr_tr), np.array(pred), np.array(raw_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 10.886\n",
      "Final Image level prediction length :  446 446\n",
      "Manual calculated Accuracy :  99.55156950672645\n",
      "===============FROM SKLEARN=================\n",
      "Accuracy = 0.9955156950672646\n",
      "AUPRC = 1.0\n",
      "All predictions saved at :  currTestSCnegP16pos/input_output_means.pkl\n"
     ]
    }
   ],
   "source": [
    "generate_metrices_from_dict_single_type(orig_labels, predictions, raw_preds, test_sub_img_dict, outputSaveBasePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(outputSaveBasePath + \"positive_prediction_result_dict.json\")\n",
    "p_data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "args = []\n",
    "for it in range(len(df)):\n",
    "    args.append(df.iloc[it, 1] + '_' + df.iloc[it, 2] + '.png')\n",
    "\n",
    "args = np.array(args)\n",
    "out_df = pd.DataFrame(columns=['Index', 'Protein_1', 'Protein_2', \n",
    "            'Protein_seq1', 'Protein_seq2', 'img_dim', 'resultant_scores'])\n",
    "\n",
    "for i in range(len(p_data)):\n",
    "    protein = pos_test_img_list[i]\n",
    "    im = Image.open(test_img_parent_loc + protein)\n",
    "    im = np.array(im)\n",
    "    height, width, _ = im.shape\n",
    "    imgDim = [len(list(range(0, height-SIZE+STRIDE, STRIDE))), len(list(range(0, width-SIZE+STRIDE, STRIDE)))]\n",
    "    str_to_end_with = protein.split('_', 1)[1]\n",
    "    idx = np.where(args == str_to_end_with)[0][0]\n",
    "    new_row = {\n",
    "        'Index' : i+1,\n",
    "        'Protein_1' : df.iloc[idx, 1],\n",
    "        'Protein_2' : df.iloc[idx, 2],\n",
    "        'Protein_seq1' : df.iloc[idx, 3],\n",
    "        'Protein_seq2' : df.iloc[idx, 4],\n",
    "        'img_dim' : imgDim,\n",
    "        'resultant_scores' : p_data[str(i + 1)]\n",
    "    }\n",
    "    out_df = out_df._append(pd.Series(new_row), ignore_index=True)\n",
    "\n",
    "out_df.to_csv(outputSaveBasePath + 'positive_result_sc_pam120_stride16.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
